{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd17286-41b7-4bbb-ad5c-b3d32c5976c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PANNs (Cnn14) model...\n",
      "Checkpoint path: /home/operator0/panns_data/Cnn14_mAP=0.431.pth\n",
      "GPU number: 1\n",
      "Model loaded on device: cuda\n",
      "\n",
      "====================\n",
      "ANALYSING DAY: LUW6952_20250701\n",
      "====================\n",
      "\n",
      "====================\n",
      "ANALYSING DAY: LUW6952_20250702\n",
      "====================\n",
      "\n",
      "====================\n",
      "ANALYSING DAY: LUW6952_20250703\n",
      "====================\n",
      "\n",
      "====================\n",
      "ANALYSING DAY: LUW6952_20250704\n",
      "====================\n",
      "\n",
      "====================\n",
      "ANALYSING DAY: LUW6952_20250705\n",
      "====================\n",
      "\n",
      "====================\n",
      "ANALYSING DAY: LUW6952_20250706\n",
      "====================\n",
      "\n",
      "====================\n",
      "ANALYSING DAY: LUW6952_20250707\n",
      "====================\n",
      "\n",
      "====================\n",
      "ANALYSING DAY: LUW6952_20250708\n",
      "====================\n",
      "Final summary file for this day already exists. Skipping.\n",
      "\n",
      "====================\n",
      "ANALYSING DAY: LUW6952_20250709\n",
      "====================\n",
      "Found 236 audio files to analyze for this day.\n",
      "--- Processing file: LUW6952_20250709_000000.wav ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 441\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;66;03m# --- Execute the Analysis ---\u001b[39;00m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     \u001b[43mmain_analysis_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 413\u001b[39m, in \u001b[36mmain_analysis_loop\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    411\u001b[39m all_daily_logs = []\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m audio_file \u001b[38;5;129;01min\u001b[39;00m audio_files_for_day:\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     file_summary, file_logs = \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpanns_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m file_summary: daily_summaries.append(file_summary)\n\u001b[32m    415\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m file_logs: all_daily_logs.extend(file_logs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 226\u001b[39m, in \u001b[36mprocess_file\u001b[39m\u001b[34m(audio_file_path, panns_model, class_labels, cfg)\u001b[39m\n\u001b[32m    223\u001b[39m log_entries = []\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     waveform_np, sr = \u001b[43mpreprocess_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtarget_sample_rate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhigh_pass_filter_hz\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     safe_const = \u001b[32m1e-9\u001b[39m\n\u001b[32m    228\u001b[39m     file_rms_avg = np.sqrt(np.mean(waveform_np**\u001b[32m2\u001b[39m)) + safe_const\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 168\u001b[39m, in \u001b[36mpreprocess_audio\u001b[39m\u001b[34m(file_path, target_sr, high_pass_cutoff)\u001b[39m\n\u001b[32m    166\u001b[39m     waveform = torch.from_numpy(waveform_np).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m    167\u001b[39m resampler = torchaudio.transforms.Resample(orig_freq=original_sr, new_freq=target_sr)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m waveform = \u001b[43mresampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m waveform_np = waveform.numpy().flatten()\n\u001b[32m    170\u001b[39m filtered_waveform = high_pass_filter(waveform_np, high_pass_cutoff, target_sr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/HydrophoneAnalysis/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/HydrophoneAnalysis/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/HydrophoneAnalysis/lib/python3.13/site-packages/torchaudio/transforms/_transforms.py:979\u001b[39m, in \u001b[36mResample.forward\u001b[39m\u001b[34m(self, waveform)\u001b[39m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.orig_freq == \u001b[38;5;28mself\u001b[39m.new_freq:\n\u001b[32m    978\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m waveform\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_sinc_resample_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43morig_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnew_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgcd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/HydrophoneAnalysis/lib/python3.13/site-packages/torchaudio/functional/functional.py:1466\u001b[39m, in \u001b[36m_apply_sinc_resample_kernel\u001b[39m\u001b[34m(waveform, orig_freq, new_freq, gcd, kernel, width)\u001b[39m\n\u001b[32m   1464\u001b[39m num_wavs, length = waveform.shape\n\u001b[32m   1465\u001b[39m waveform = torch.nn.functional.pad(waveform, (width, width + orig_freq))\n\u001b[32m-> \u001b[39m\u001b[32m1466\u001b[39m resampled = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43morig_freq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1467\u001b[39m resampled = resampled.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).reshape(num_wavs, -\u001b[32m1\u001b[39m)\n\u001b[32m   1468\u001b[39m target_length = torch.ceil(torch.as_tensor(new_freq * length / orig_freq)).long()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Main script for acoustic anomaly detection in hydrophone recordings.\n",
    "#\n",
    "# AI contributions:\n",
    "# - Assisted with major refactoring for memory efficiency, code quality, and readability.\n",
    "# - Implemented advanced features for detailed logging, anomaly explanation, and results validation.\n",
    "#\n",
    "\n",
    "import datetime\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "from scipy.signal import butter, sosfilt\n",
    "from sklearn.cluster import DBSCAN\n",
    "import os\n",
    "import glob\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend to prevent memory leaks\n",
    "import matplotlib.pyplot as plt\n",
    "from panns_inference import AudioTagging\n",
    "import gc\n",
    "import csv\n",
    "import pathlib\n",
    "\n",
    "\n",
    "LEG = 'LEG9_S20250708_PSTATSRAADLEHMKUHL' #'LEG10_S20250805_PSTATSRAADLEHMKUHL'\n",
    "MONTH = str(7) # set to the month you want to anlyse\n",
    "# --- Configuration ---\n",
    "config = {\n",
    "    # --- Paths ---\n",
    "    \n",
    "    \"audio_folder_path\": ...,\n",
    "    \"output_folder\": ...,\n",
    "    \"audio_clips_subfolder\": \"audio_clips\",\n",
    "    \"loud_clips_subfolder\": \"high_quality_clips\",\n",
    "    \"hq_plots_subfolder\": \"high_quality_plots\",\n",
    "    \"std_plots_subfolder\": \"standard_plots\",\n",
    "\n",
    "    # --- Anomaly Quality Control ---\n",
    "    \"separate_loud_anomalies\": True,\n",
    "    \"loudness_ratio_threshold\": 2.5,\n",
    "\n",
    "    # --- Audio Processing & Visualization ---\n",
    "    \"chunk_seconds\": 3.0,\n",
    "    \"pre_context_seconds\": 5.0, # Seconds of audio to save before the event group\n",
    "    \"post_context_seconds\": 7.0, # Seconds of audio to save after the event group (5s + 2s extra)\n",
    "    \"plot_context_seconds\": 2.0, # Shorter context for a more zoomed-in plot\n",
    "    \"target_sample_rate\": 32000,\n",
    "    \"high_pass_filter_hz\": 2000,\n",
    "\n",
    "\n",
    "\n",
    "    # --- Instrument Filtering ---\n",
    "    \"instrument_frequencies_hz\": [38000, 75000, 80000], # Frequencies of EK80 and ADCP\n",
    "    \"frequency_tolerance_hz\": 250,      # Search tolerance around each frequency\n",
    "    \"instrument_peak_ratio\": 8.0,       # A ping's peak must be 8x the average spectral magnitude to be filtered\n",
    "    \"instrument_energy_threshold\": 0.30,\n",
    "\n",
    "\n",
    "    # --- Clustering & Grouping ---\n",
    "    \"dbscan_eps\": 2.5,\n",
    "    \"dbscan_min_samples\": 4,\n",
    "    \"temporal_grouping_seconds\": 15.0,\n",
    "    \"min_anomalies_in_group\": 2,\n",
    "\n",
    "    # --- Model Prediction Filtering ---\n",
    "    \"validate_predictions\": True,\n",
    "    \"target_labels\": ['Whale', 'Dolphin', 'Whistling', 'Chirp tone', 'Animal'],\n",
    "    \"avoid_labels\" : ['Stream','Water','Pour', 'Frying (food)','Drip','Boiling','Raindrop',],\n",
    "    \"min_confidence\": 0.25,\n",
    "\n",
    "    # --- General ---\n",
    "    \"hydrophone_id\": \"LUW6952\"\n",
    "}\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def high_pass_filter(data, cutoff, fs, order=5):\n",
    "    \"\"\"Applies a high-pass filter to the data.\"\"\"\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    if normal_cutoff >= 1.0: return data\n",
    "    sos = butter(order, normal_cutoff, btype='high', analog=False, output='sos')\n",
    "    y = sosfilt(sos, data)\n",
    "    return y\n",
    "\n",
    "def calculate_aliased_frequency(original_freq, sample_rate):\n",
    "    \"\"\"Calculates the aliased frequency after downsampling.\"\"\"\n",
    "    nyquist = sample_rate / 2\n",
    "    while original_freq > nyquist:\n",
    "        original_freq = abs(original_freq - sample_rate)\n",
    "    return original_freq\n",
    "\n",
    "def is_instrument_ping_new(chunk, sr, unwanted_freqs, tolerance_hz, energy_threshold=0.30):\n",
    "    \"\"\"Checks if a chunk's energy is concentrated in unwanted frequency bands.\"\"\"\n",
    "    if np.sum(np.abs(chunk)) == 0: return False\n",
    "    spectrum = np.abs(np.fft.rfft(chunk))\n",
    "    freq_axis = np.fft.rfftfreq(len(chunk), 1/sr)\n",
    "    total_energy = np.sum(spectrum**2)\n",
    "    if total_energy == 0: return False\n",
    "    ping_energy = 0\n",
    "    for freq_hz in unwanted_freqs:\n",
    "        band_mask = (freq_axis >= freq_hz - tolerance_hz) & (freq_axis <= freq_hz + tolerance_hz)\n",
    "        ping_energy += np.sum(spectrum[band_mask]**2)\n",
    "    energy_ratio = ping_energy / total_energy\n",
    "    return energy_ratio > energy_threshold\n",
    "\n",
    "\n",
    "def is_instrument_ping(chunk, sr, aliased_freqs, freq_tolerance, peak_ratio_threshold):\n",
    "    \"\"\"\n",
    "    Detects an instrument ping by looking for a dominant spectral peak in predefined frequency bands.\n",
    "\n",
    "    This method is more robust than simple energy ratios because it specifically looks for a\n",
    "    signal's \"peakiness,\" which is characteristic of narrow-band pings from echosounders and ADCPs.\n",
    "\n",
    "    Args:\n",
    "        chunk (np.ndarray): The audio chunk to analyze.\n",
    "        sr (int): The sample rate of the chunk.\n",
    "        aliased_freqs (list): A list of instrument frequencies, already corrected for aliasing.\n",
    "        freq_tolerance (float): The tolerance in Hz to search around each instrument frequency.\n",
    "        peak_ratio_threshold (float): The peak magnitude must be this many times greater than the\n",
    "                                      average magnitude of the spectrum to be considered a ping.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if a ping is detected, False otherwise.\n",
    "    \"\"\"\n",
    "    if np.sum(np.abs(chunk)) == 0:\n",
    "        return False\n",
    "\n",
    "    # Calculate the magnitude spectrum using Fast Fourier Transform\n",
    "    spectrum = np.abs(np.fft.rfft(chunk))\n",
    "    freq_axis = np.fft.rfftfreq(len(chunk), 1/sr)\n",
    "\n",
    "    # Calculate the average magnitude of the entire spectrum for comparison\n",
    "    avg_magnitude = np.mean(spectrum)\n",
    "    if avg_magnitude == 0:\n",
    "        return False\n",
    "\n",
    "    # Check each specified instrument frequency band for a dominant peak\n",
    "    for freq_hz in aliased_freqs:\n",
    "        # Define the frequency band to inspect\n",
    "        band_mask = (freq_axis >= freq_hz - freq_tolerance) & (freq_axis <= freq_hz + freq_tolerance)\n",
    "        \n",
    "        if np.any(band_mask):\n",
    "            # Find the maximum peak within this specific band\n",
    "            band_peak_magnitude = np.max(spectrum[band_mask])\n",
    "            \n",
    "            # Key step: Check if this peak is significantly stronger than the average\n",
    "            if band_peak_magnitude > (avg_magnitude * peak_ratio_threshold):\n",
    "                # A dominant ping was found, no need to check other frequencies\n",
    "                return True\n",
    "\n",
    "    # No dominant pings were found in any of the specified bands\n",
    "    return False\n",
    "\n",
    "def preprocess_audio(file_path, target_sr, high_pass_cutoff):\n",
    "    \"\"\"Loads, filters, and resamples a single audio file.\"\"\"\n",
    "    try:\n",
    "        waveform, original_sr = torchaudio.load(file_path)\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    except Exception as e:\n",
    "        print(f\"    - Torchaudio failed: {e}. Falling back to librosa.\")\n",
    "        waveform_np, original_sr = librosa.load(file_path, sr=None, mono=True)\n",
    "        waveform = torch.from_numpy(waveform_np).unsqueeze(0)\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=original_sr, new_freq=target_sr)\n",
    "    waveform = resampler(waveform)\n",
    "    waveform_np = waveform.numpy().flatten()\n",
    "    filtered_waveform = high_pass_filter(waveform_np, high_pass_cutoff, target_sr)\n",
    "    return filtered_waveform, target_sr\n",
    "\n",
    "def create_zoomed_event_plot(full_waveform, sr, file_name, group, plot_directory, cfg):\n",
    "    \"\"\"Generates a spectrogram zoomed in on a single event group with context.\"\"\"\n",
    "    plot_context = cfg['plot_context_seconds']\n",
    "    group_start_time = group[0]['time']\n",
    "    group_end_time = group[-1]['time'] + cfg['chunk_seconds']\n",
    "    \n",
    "    # Determine time and sample boundaries for the plot\n",
    "    plot_start_time = group_start_time - plot_context\n",
    "    plot_end_time = group_end_time + plot_context\n",
    "    start_sample = max(0, int(plot_start_time * sr))\n",
    "    end_sample = min(len(full_waveform), int(plot_end_time * sr))\n",
    "    \n",
    "    # Slice the audio to the zoomed-in window\n",
    "    zoomed_clip = full_waveform[start_sample:end_sample]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(18, 6)) \n",
    "    n_fft, hop_length = 2048, 512\n",
    "    S = librosa.feature.melspectrogram(y=zoomed_clip, sr=sr, n_mels=128, n_fft=n_fft, hop_length=hop_length)\n",
    "    log_S = librosa.power_to_db(S, ref=np.max)\n",
    "    \n",
    "    librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel', ax=ax, hop_length=hop_length, cmap='magma')\n",
    "    fig.colorbar(ax.collections[0], ax=ax, format='%+2.0f dB', label='Intensity (dB)')\n",
    "\n",
    "    title = f\"Zoomed Event in {file_name}\\nOriginal Time: {group_start_time:.1f}s - {group_end_time:.1f}s\"\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    predictions_text = f\"--- Predictions for Event ---\\n\"\n",
    "    seen_predictions = set()\n",
    "    for item in group:\n",
    "        main_label, main_prob = item['top_3'][0]\n",
    "        if main_label not in seen_predictions:\n",
    "            predictions_text += f\"- {main_label}: {main_prob:.2f}\\n\"\n",
    "            seen_predictions.add(main_label)\n",
    "    \n",
    "    fig.text(0.02, 0.02, predictions_text, fontsize=9, wrap=True, verticalalignment='bottom')\n",
    "    plt.tight_layout(rect=[0.0, 0.1, 1, 0.95])\n",
    "    \n",
    "    output_filename = f\"{os.path.splitext(file_name)[0]}_event_at_{group_start_time:.0f}s_zoomed.png\"\n",
    "    output_path = os.path.join(plot_directory, output_filename)\n",
    "    plt.savefig(output_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "    return output_path\n",
    "\n",
    "# --- Analysis Functions ---\n",
    "\n",
    "def process_file(audio_file_path, panns_model, class_labels, cfg):\n",
    "    \"\"\"Processes a single audio file, returning a summary and detailed logs.\"\"\"\n",
    "    file_name = os.path.basename(audio_file_path)\n",
    "    print(f\"--- Processing file: {file_name} ---\")\n",
    "    log_entries = []\n",
    "\n",
    "    try:\n",
    "        waveform_np, sr = preprocess_audio(audio_file_path, cfg['target_sample_rate'], cfg['high_pass_filter_hz'])\n",
    "        safe_const = 1e-9\n",
    "        file_rms_avg = np.sqrt(np.mean(waveform_np**2)) + safe_const\n",
    "        \n",
    "        chunk_samples = int(cfg['chunk_seconds'] * sr)\n",
    "        num_chunks = len(waveform_np) // chunk_samples\n",
    "        if num_chunks == 0:\n",
    "            print(\"  - File is too short to be analyzed. Skipping.\")\n",
    "            return None, log_entries\n",
    "\n",
    "        aliased_instrument_freqs = [calculate_aliased_frequency(f, sr) for f in cfg['instrument_frequencies_hz']]\n",
    "        \n",
    "        print(f\"  - Extracting features from {num_chunks} chunks...\")\n",
    "        all_embeddings, all_clipwise_outputs = [], []\n",
    "        for i in range(num_chunks):\n",
    "            chunk = waveform_np[i * chunk_samples:(i + 1) * chunk_samples]\n",
    "            if len(chunk) < chunk_samples: chunk = np.pad(chunk, (0, chunk_samples - len(chunk)))\n",
    "            clipwise_output, embedding = panns_model.inference(chunk[np.newaxis, :])\n",
    "            all_embeddings.append(embedding)\n",
    "            all_clipwise_outputs.append(clipwise_output[0])\n",
    "        \n",
    "        feature_matrix = np.array(all_embeddings).reshape(num_chunks, -1)\n",
    "        del all_embeddings\n",
    "        \n",
    "        print(\"  - Clustering features to find initial anomalies...\")\n",
    "        db = DBSCAN(eps=cfg['dbscan_eps'], min_samples=cfg['dbscan_min_samples']).fit(feature_matrix)\n",
    "        anomaly_indices = np.where(db.labels_ == -1)[0]\n",
    "        del feature_matrix\n",
    "\n",
    "        if len(anomaly_indices) == 0:\n",
    "            print(\"No initial anomalies detected by clustering.\")\n",
    "            return None, log_entries\n",
    "        \n",
    "        print(f\"  - Found {len(anomaly_indices)} potential anomalies. Validating...\")\n",
    "        validated_anomalies_temp = []\n",
    "        for i in anomaly_indices:\n",
    "            start_time = i * cfg['chunk_seconds']\n",
    "            anomaly_chunk = waveform_np[i*chunk_samples:(i+1)*chunk_samples]\n",
    "            prediction = all_clipwise_outputs[i]\n",
    "            top_idx, top_prob = np.argmax(prediction), np.max(prediction)\n",
    "            top_label = class_labels[top_idx]\n",
    "            \n",
    "            chunk_rms = np.sqrt(np.mean(anomaly_chunk**2))\n",
    "            loudness_ratio = chunk_rms / file_rms_avg\n",
    "            \n",
    "            if is_instrument_ping_new(anomaly_chunk, sr, aliased_instrument_freqs, cfg['frequency_tolerance_hz'], cfg['instrument_energy_threshold']):\n",
    "                log_entries.append({'file': file_name, 'timestamp_s': f\"{start_time:.2f}\", 'status': 'DISCARDED', 'reason': 'Instrument Ping', 'top_prediction': top_label, 'confidence': f\"{top_prob:.2f}\", 'details': f\"LoudnessRatio={loudness_ratio:.2f}\"})\n",
    "                continue\n",
    "            if is_instrument_ping(anomaly_chunk, sr, aliased_instrument_freqs, cfg['frequency_tolerance_hz'], cfg['instrument_peak_ratio']):\n",
    "                log_entries.append({'file': file_name, 'timestamp_s': f\"{start_time:.2f}\", 'status': 'DISCARDED', 'reason': 'Instrument Ping', 'top_prediction': top_label, 'confidence': f\"{top_prob:.2f}\", 'details': f\"LoudnessRatio={loudness_ratio:.2f}\"})\n",
    "                continue\n",
    "            \n",
    "            is_valid = not cfg['validate_predictions'] or not (top_label in cfg['avoid_labels'])\n",
    "            if not is_valid:\n",
    "                log_entries.append({'file': file_name, 'timestamp_s': f\"{start_time:.2f}\", 'status': 'DISCARDED', 'reason': 'Failed Validation', 'top_prediction': top_label, 'confidence': f\"{top_prob:.2f}\", 'details': f\"LoudnessRatio={loudness_ratio:.2f}\"})\n",
    "                continue\n",
    "\n",
    "            top_3_indices = np.argsort(prediction)[::-1][:3]\n",
    "            top_3_preds = [(class_labels[k], prediction[k]) for k in top_3_indices]\n",
    "            validated_anomalies_temp.append({\"time\": start_time, \"top_3\": top_3_preds, \"loudness_ratio\": loudness_ratio})\n",
    "        \n",
    "        del all_clipwise_outputs\n",
    "        if not validated_anomalies_temp:\n",
    "            print(\"No anomalies passed validation.\")\n",
    "            return None, log_entries\n",
    "\n",
    "        validated_anomalies_temp.sort(key=lambda x: x['time'])\n",
    "        event_groups = []\n",
    "        current_group = [validated_anomalies_temp[0]]\n",
    "        for i in range(1, len(validated_anomalies_temp)):\n",
    "            if validated_anomalies_temp[i]['time'] - current_group[-1]['time'] <= cfg['temporal_grouping_seconds']:\n",
    "                current_group.append(validated_anomalies_temp[i])\n",
    "            else:\n",
    "                if len(current_group) >= cfg['min_anomalies_in_group']: event_groups.append(current_group)\n",
    "                current_group = [validated_anomalies_temp[i]]\n",
    "        if len(current_group) >= cfg['min_anomalies_in_group']: event_groups.append(current_group)\n",
    "\n",
    "        if not event_groups:\n",
    "            print(\"No significant event groups detected (anomalies were too isolated).\")\n",
    "            for anom in validated_anomalies_temp:\n",
    "                log_entries.append({'file': file_name, 'timestamp_s': f\"{anom['time']:.2f}\", 'status': 'DISCARDED', 'reason': 'Isolated Anomaly', 'top_prediction': anom['top_3'][0][0], 'confidence': f\"{anom['top_3'][0][1]:.2f}\", 'details': f\"LoudnessRatio={anom['loudness_ratio']:.2f}\"})\n",
    "            return None, log_entries\n",
    "        \n",
    "        print(f\"Found {len(event_groups)} distinct event group(s). Generating outputs...\")\n",
    "        \n",
    "        hq_plots_path = os.path.join(cfg['output_folder'], cfg['hq_plots_subfolder'])\n",
    "        std_plots_path = os.path.join(cfg['output_folder'], cfg['std_plots_subfolder'])\n",
    "\n",
    "        plot_path_map = {}\n",
    "        for group in event_groups:\n",
    "            is_high_quality = any(anom['loudness_ratio'] >= cfg['loudness_ratio_threshold'] for anom in group)\n",
    "            plot_dir = hq_plots_path if is_high_quality and cfg['separate_loud_anomalies'] else std_plots_path\n",
    "            \n",
    "            plot_path = create_zoomed_event_plot(waveform_np, sr, file_name, group, plot_dir, cfg)\n",
    "            print(f\"    - Zoomed spectrogram saved: {os.path.basename(plot_path)}\")\n",
    "            for anom in group:\n",
    "                plot_path_map[anom['time']] = plot_path\n",
    "        \n",
    "        for anom in validated_anomalies_temp:\n",
    "            log_entry = {'file': file_name, 'timestamp_s': f\"{anom['time']:.2f}\", 'top_prediction': anom['top_3'][0][0], 'confidence': f\"{anom['top_3'][0][1]:.2f}\", 'details': f\"LoudnessRatio={anom['loudness_ratio']:.2f}\"}\n",
    "            plot_path = plot_path_map.get(anom['time'])\n",
    "            if plot_path:\n",
    "                log_entry.update({'status': 'KEPT', 'reason': 'Part of Event Group', 'spectrogram_path': plot_path})\n",
    "            else:\n",
    "                log_entry.update({'status': 'DISCARDED', 'reason': 'Isolated Anomaly', 'spectrogram_path': ''})\n",
    "            log_entries.append(log_entry)\n",
    "        \n",
    "        high_quality_event_count = sum(1 for group in event_groups if any(anom['loudness_ratio'] >= cfg['loudness_ratio_threshold'] for anom in group))\n",
    "        \n",
    "        base_audio_path = os.path.join(cfg['output_folder'], cfg['audio_clips_subfolder'])\n",
    "        hq_audio_path = os.path.join(cfg['output_folder'], cfg['loud_clips_subfolder'])\n",
    "        for group in event_groups:\n",
    "            is_high_quality = any(anom['loudness_ratio'] >= cfg['loudness_ratio_threshold'] for anom in group)\n",
    "            output_path = hq_audio_path if cfg['separate_loud_anomalies'] and is_high_quality else base_audio_path\n",
    "            start_time = group[0]['time']\n",
    "            clip_filename = f\"{os.path.splitext(file_name)[0]}_event_clip_{start_time:.0f}s.wav\"\n",
    "            start_sample = max(0, int((start_time - cfg['pre_context_seconds']) * sr))\n",
    "            end_sample = min(len(waveform_np), int((group[-1]['time'] + cfg['chunk_seconds'] + cfg['post_context_seconds']) * sr))\n",
    "            torchaudio.save(os.path.join(output_path, clip_filename), torch.from_numpy(waveform_np[start_sample:end_sample]).unsqueeze(0), sr)\n",
    "            \n",
    "        file_summary = {'filename': file_name, 'total_events': len(event_groups), 'high_quality_events': high_quality_event_count}\n",
    "        return file_summary, log_entries\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FAILED to process file {file_name}. Error: {e}\")\n",
    "        return None, log_entries\n",
    "    finally:\n",
    "        if 'waveform_np' in locals(): del waveform_np\n",
    "        gc.collect()\n",
    "\n",
    "def main_analysis_loop():\n",
    "    \"\"\"Main execution block: sets up directories, loads model, and loops through days.\"\"\"\n",
    "    os.makedirs(config['output_folder'], exist_ok=True)\n",
    "    os.makedirs(os.path.join(config['output_folder'], config['audio_clips_subfolder']), exist_ok=True)\n",
    "    os.makedirs(os.path.join(config['output_folder'], config['std_plots_subfolder']), exist_ok=True)\n",
    "    if config['separate_loud_anomalies']:\n",
    "        os.makedirs(os.path.join(config['output_folder'], config['loud_clips_subfolder']), exist_ok=True)\n",
    "        os.makedirs(os.path.join(config['output_folder'], config['hq_plots_subfolder']), exist_ok=True)\n",
    "    \n",
    "    print(\"Loading PANNs (Cnn14) model...\")\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    panns_model = AudioTagging(checkpoint_path=None, device=device)\n",
    "    class_labels = panns_model.labels\n",
    "    print(f\"Model loaded on device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "    today_str = datetime.date.today().strftime('%Y%m%d')\n",
    "\n",
    "    days_to_process = [f\"{d:02d}\" for d in range(1, 31)]\n",
    "    #days_to_process = ['09'] \n",
    "    # kept for validation purposes - on the 9th we have confirmed dolphin sightings\n",
    "    for day in days_to_process:\n",
    "\n",
    "\n",
    "        processing_date_str = f\"20250{MONTH}{day}\"\n",
    "        is_today = (processing_date_str == today_str)\n",
    "        date_pattern = f\"{config['hydrophone_id']}_{processing_date_str}\"\n",
    "        print(f\"\\n{'='*20}\\nANALYSING DAY: {date_pattern}\\n{'='*20}\")\n",
    "\n",
    "        summary_filename = f\"{date_pattern}_daily_summary.csv\"\n",
    "        log_filename = f\"{date_pattern}_detailed_log.csv\"\n",
    "\n",
    "        # check if we're running the scrip TODAY (we assume the data isn't collected fully so we want to have an incomplete analysis so we can run again later\n",
    "        if is_today:\n",
    "            print(f\"INFO: Processing for the current day. Outputs will be marked as INCOMPLETE.\")\n",
    "            summary_filename = f\"{date_pattern}_daily_summary_INCOMPLETE.csv\"\n",
    "            log_filename = f\"{date_pattern}_detailed_log_INCOMPLETE.csv\"\n",
    "\n",
    "        summary_file_path = os.path.join(config['output_folder'], summary_filename)\n",
    "        log_file_path = os.path.join(config['output_folder'], log_filename)\n",
    "\n",
    "        # Skip if a FINAL summary exists for a past day. Allows re-running and overwriting for \"today\".\n",
    "        final_summary_path = os.path.join(config['output_folder'], f\"{date_pattern}_daily_summary.csv\")\n",
    "        if not is_today and os.path.exists(final_summary_path):\n",
    "            print(f\"Final summary file for this day already exists. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        search_pattern = os.path.join(config['audio_folder_path'], f\"*{date_pattern}*.wav\")\n",
    "        audio_files_for_day = sorted(glob.glob(search_pattern))\n",
    "        if not audio_files_for_day: continue\n",
    "        \n",
    "        print(f\"Found {len(audio_files_for_day)} audio files to analyze for this day.\")\n",
    "        \n",
    "        daily_summaries = []\n",
    "        all_daily_logs = []\n",
    "        for audio_file in audio_files_for_day:\n",
    "            file_summary, file_logs = process_file(audio_file, panns_model, class_labels, config)\n",
    "            if file_summary: daily_summaries.append(file_summary)\n",
    "            if file_logs: all_daily_logs.extend(file_logs)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        # Write detailed log to CSV\n",
    "        if all_daily_logs:\n",
    "            log_file_path = os.path.join(config['output_folder'], f\"{date_pattern}_detailed_log.csv\")\n",
    "            log_fieldnames = ['file', 'timestamp_s', 'status', 'reason', 'top_prediction', 'confidence', 'details', 'spectrogram_path']\n",
    "            with open(log_file_path, 'w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=log_fieldnames, extrasaction='ignore')\n",
    "                writer.writeheader()\n",
    "                writer.writerows(all_daily_logs)\n",
    "\n",
    "        # Write high-level summary to CSV\n",
    "        if daily_summaries:\n",
    "            print(f\"\\n--- Day {day} Complete ---\")\n",
    "            print(f\"Found events in {len(daily_summaries)} files. Saving summary to CSV.\")\n",
    "            summary_fieldnames = ['filename', 'total_events', 'high_quality_events']\n",
    "            with open(summary_file_path, 'w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=summary_fieldnames)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(daily_summaries)\n",
    "        else:\n",
    "            print(f\"\\n--- Day {day} Complete --- \\nNo significant events found.\")\n",
    "\n",
    "# --- Execute the Analysis ---\n",
    "if __name__ == \"__main__\":\n",
    "    main_analysis_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e57729-252e-4188-9416-7fb56f60d0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
